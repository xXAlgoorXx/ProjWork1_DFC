{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to convert a Pytroch model to a HEF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lukasschoepf/Documents/ProjWork1_DFC/python\n"
     ]
    }
   ],
   "source": [
    "# import the ClientRunner class from the hailo_sdk_client package\n",
    "from hailo_sdk_client import ClientRunner\n",
    "import onnx\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "import open_clip\n",
    "from pathlib import Path\n",
    "from onnxsim import simplify\n",
    "from PIL import Image\n",
    "\n",
    "# Own modules\n",
    "print(Path.cwd())\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "os.chdir(str(Path.cwd().parent))\n",
    "import pathsToFolders as ptf #Controlls all paths #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths and names for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_hw_arch = \"hailo8l\"\n",
    "\n",
    "har_path = ptf.HarPath\n",
    "hef_path = ptf.Hefpath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ONNX file\n",
    "\n",
    "Diffrent for CLIP / TinyCLIP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 288\n",
      "Simple Model saved at hailoDFC/models/RN50x4_simple.onnx\n"
     ]
    }
   ],
   "source": [
    "model_name = \"RN50x4\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "model, preprocess = clip.load(model_name, device=device)\n",
    "\n",
    "vit = model.visual\n",
    "input_pixels = model.visual.input_resolution\n",
    "input_image = torch.rand(1, 3, input_pixels, input_pixels)\n",
    "\n",
    "input_image = preprocess(Image.open(\"hailoDFC/pics/car-967387_1280.png\")).unsqueeze(0).to(device)\n",
    "onnx_path = str(ptf.onnxFolder/ f\"{model_name}.onnx\")\n",
    "print(f\"Input: {input_pixels}\")\n",
    "vit.eval()\n",
    "torch.onnx.export(vit,\n",
    "            input_image,\n",
    "            onnx_path,\n",
    "            opset_version=14,\n",
    "            do_constant_folding=True,)\n",
    "\n",
    "# Simplify the ONNX model to resolve mismatches\n",
    "simplified_model, check = simplify(onnx_path)\n",
    "model_path_simple = f\"hailoDFC/models/{model_name}_simple.onnx\"\n",
    "\n",
    "# Save the simplified model\n",
    "onnx.save(simplified_model, model_path_simple)\n",
    "\n",
    "print(f\"Simple Model saved at {model_path_simple}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For TinyCLIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukasschoepf/Documents/ProjWork1_DFC/src/open_clip/factory.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 224\n",
      "Simple Model saved at hailoDFC/models/TinyRN_simple.onnx\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TinyCLIP-ResNet-19M-Text-19M\" # TinyCLIP-ResNet-19M-Text-19M or TinyCLIP-ResNet-30M-Text-29M\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "model, *_ = open_clip.create_model_and_transforms(model_name,pretrained=str(ptf.tinyClipModels / f\"{model_name}-LAION400M.pt\"))\n",
    "\n",
    "vit = model.visual \n",
    "input_pixels = model.visual.image_size\n",
    "\n",
    "print(f\"Input: {input_pixels}\")\n",
    "input_image = torch.rand(1, 3, input_pixels, input_pixels)\n",
    "# input_image = preprocess(Image.open(\"hailoDFC/pics/car-967387_1280.png\")).unsqueeze(0).to(device)\n",
    "\n",
    "onnx_path = str(ptf.onnxFolder/ f\"{model_name}.onnx\")\n",
    "vit.eval()\n",
    "torch.onnx.export(vit,\n",
    "         input_image,\n",
    "         onnx_path,\n",
    "         opset_version=14,\n",
    "         do_constant_folding=True,)\n",
    "\n",
    "# Simplify the ONNX model to resolve mismatches\n",
    "simplified_model, check = simplify(onnx_path)\n",
    "model_name = \"TinyRN\"\n",
    "model_path_simple = f\"hailoDFC/models/{model_name}_simple.onnx\"\n",
    "\n",
    "# Save the simplified model\n",
    "onnx.save(simplified_model, model_path_simple)\n",
    "\n",
    "print(f\"Simple Model saved at {model_path_simple}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use model from Hailo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_simple = \"hailoDFC/models/baseAndSimple/TinyRN_simple.onnx\"\n",
    "model_name = \"tinyCLIP_RN50\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onnx to Har\n",
    "\n",
    "Changes to onnx_graph.py\n",
    "\n",
    "```python\n",
    "def get_spatial_unflatten_reshape_info(self):\n",
    "        if self.op == \"Reshape\": # if self.op != \"Reshape\":\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take model from hailoDFC/models/baseAndSimple/TinyRN_simple.onnx\n",
      "[info] Translation started on ONNX model tinyCLIP_RN50\n",
      "[info] Restored ONNX model tinyCLIP_RN50 (completion time: 00:00:00.10)\n",
      "[info] Extracted ONNXRuntime meta-data for Hailo model (completion time: 00:00:00.33)\n",
      "[info] Simplified ONNX model for a parsing retry attempt (completion time: 00:00:00.65)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/sdk_backend/parser/parser.py:235\u001b[0m, in \u001b[0;36mParser.translate_onnx_model\u001b[0;34m(self, model, net_name, start_node_names, end_node_names, net_input_shapes, augmented_path, disable_shape_inference, disable_rt_metadata_extraction, net_input_format, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     parsing_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_onnx_model_to_hn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_net_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_node_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_node_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_node_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_node_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnet_input_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet_input_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_shape_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_shape_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnet_input_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet_input_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/sdk_backend/parser/parser.py:316\u001b[0m, in \u001b[0;36mParser._parse_onnx_model_to_hn\u001b[0;34m(self, onnx_model, net_name, start_node_names, end_node_names, net_input_shapes, disable_shape_inference, net_input_format, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mONNX shape inference failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_model_to_hn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_node_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_node_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnn_framework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNNFramework\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mONNX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet_input_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet_input_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/sdk_backend/parser/parser.py:367\u001b[0m, in \u001b[0;36mParser.parse_model_to_hn\u001b[0;34m(self, model, values, net_name, start_node_names, end_node_names, nn_framework, output_shapes, net_input_format, rename_layers_by_blocks)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendRuntimeException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported NN framework \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnn_framework\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 367\u001b[0m fuser \u001b[38;5;241m=\u001b[39m HailoNNFuser(\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, net_name, converter\u001b[38;5;241m.\u001b[39mend_node_names)\n\u001b[1;32m    368\u001b[0m hailo_nn \u001b[38;5;241m=\u001b[39m fuser\u001b[38;5;241m.\u001b[39mconvert_model()\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/model_translator/translator.py:83\u001b[0m, in \u001b[0;36mHailoNNConverter.convert_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_bn_ops_in_training()\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_layers_connections()\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/model_translator/edge_nn_translator.py:40\u001b[0m, in \u001b[0;36mEdgeNNConverter._create_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_vertices_info()\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_direct_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_processed_vertices()\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/model_translator/edge_nn_translator.py:122\u001b[0m, in \u001b[0;36mEdgeNNConverter._add_direct_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing vertex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvertex\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_layer_callback_from_vertex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_visited_states[vertex] \u001b[38;5;241m=\u001b[39m VertexState\u001b[38;5;241m.\u001b[39mPROCESSED\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/model_translator/onnx_translator/onnx_translator.py:389\u001b[0m, in \u001b[0;36mONNXConverter._layer_callback_from_vertex\u001b[0;34m(self, vertex)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    374\u001b[0m     vertex\u001b[38;5;241m.\u001b[39mis_f_to_w_transpose_reshape()\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m vertex\u001b[38;5;241m.\u001b[39mis_flat_to_frames_reshape()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m vertex\u001b[38;5;241m.\u001b[39mis_partial_groups_to_spatial_flatten()\n\u001b[1;32m    388\u001b[0m ):\n\u001b[0;32m--> 389\u001b[0m     consumed_vertices, should_assign_vertex_to_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_format_conversion_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/model_translator/onnx_translator/onnx_translator.py:1451\u001b[0m, in \u001b[0;36mONNXConverter._create_format_conversion_layer\u001b[0;34m(self, vertex)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1446\u001b[0m     vertex\u001b[38;5;241m.\u001b[39mis_spatial_unflatten()\n\u001b[1;32m   1447\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vertex\u001b[38;5;241m.\u001b[39mis_f_to_w_transpose_reshape()\n\u001b[1;32m   1448\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vertex\u001b[38;5;241m.\u001b[39mis_merge_windows_to_input_reshape_chain()\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vertex\u001b[38;5;241m.\u001b[39mis_split_input_to_windows_reshape_chain()\n\u001b[1;32m   1450\u001b[0m ):\n\u001b[0;32m-> 1451\u001b[0m     consumed_vertices, shapes, expand_sizes \u001b[38;5;241m=\u001b[39m \u001b[43mvertex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_spatial_unflatten_reshape_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1452\u001b[0m     format_conversion \u001b[38;5;241m=\u001b[39m FormatConversionType\u001b[38;5;241m.\u001b[39mspatial_reshape\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/model_translator/onnx_translator/onnx_graph.py:3602\u001b[0m, in \u001b[0;36mONNXGraphNode.get_spatial_unflatten_reshape_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3601\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m consumed_vertices[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mget_output_shapes()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 3602\u001b[0m spatial_reshape_sizes \u001b[38;5;241m=\u001b[39m [output_shape[\u001b[38;5;241m1\u001b[39m], \u001b[43moutput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[1;32m   3603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m consumed_vertices, [output_shape], spatial_reshape_sizes\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m runner \u001b[38;5;241m=\u001b[39m ClientRunner(hw_arch\u001b[38;5;241m=\u001b[39mchosen_hw_arch)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTake model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path_simple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m hn, npz \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate_onnx_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path_simple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m hailo_model_har_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(har_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_hailo_model.har\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m runner\u001b[38;5;241m.\u001b[39msave_har(hailo_model_har_name)\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_common/states/states.py:16\u001b[0m, in \u001b[0;36mallowed_states.<locals>.wrap.<locals>.wrapped_func\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m states:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidStateException(\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe execution of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not available under the state: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     )\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/runner/client_runner.py:1158\u001b[0m, in \u001b[0;36mClientRunner.translate_onnx_model\u001b[0;34m(self, model, net_name, start_node_names, end_node_names, net_input_shapes, augmented_path, disable_shape_inference, disable_rt_metadata_extraction, net_input_format, **kwargs)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;124;03mDFC API for parsing an ONNX model. This creates a runner with loaded HN (model) and\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;124;03mparameters.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \n\u001b[1;32m   1156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m parser \u001b[38;5;241m=\u001b[39m Parser()\n\u001b[0;32m-> 1158\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate_onnx_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_node_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_node_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_node_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_node_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet_input_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet_input_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugmented_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugmented_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_shape_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_shape_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_rt_metadata_extraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_rt_metadata_extraction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet_input_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet_input_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize_parsing(parser\u001b[38;5;241m.\u001b[39mreturn_data)\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/sdk_backend/parser/parser.py:276\u001b[0m, in \u001b[0;36mParser.translate_onnx_model\u001b[0;34m(self, model, net_name, start_node_names, end_node_names, net_input_shapes, augmented_path, disable_shape_inference, disable_rt_metadata_extraction, net_input_format, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m     milestone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_time_milestone(start_time)\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimplified ONNX model for a parsing retry attempt (completion time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmilestone\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 276\u001b[0m     parsing_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_onnx_model_to_hn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimplified_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_net_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_node_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_node_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_node_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_node_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnet_input_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet_input_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_shape_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_shape_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnet_input_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet_input_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m milestone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_time_milestone(start_time)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslation completed on ONNX model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_net_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (completion time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmilestone\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/sdk_backend/parser/parser.py:316\u001b[0m, in \u001b[0;36mParser._parse_onnx_model_to_hn\u001b[0;34m(self, onnx_model, net_name, start_node_names, end_node_names, net_input_shapes, disable_shape_inference, net_input_format, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mONNX shape inference failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_model_to_hn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_node_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_node_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnn_framework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNNFramework\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mONNX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet_input_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet_input_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/sdk_backend/parser/parser.py:367\u001b[0m, in \u001b[0;36mParser.parse_model_to_hn\u001b[0;34m(self, model, values, net_name, start_node_names, end_node_names, nn_framework, output_shapes, net_input_format, rename_layers_by_blocks)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendRuntimeException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported NN framework \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnn_framework\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 367\u001b[0m fuser \u001b[38;5;241m=\u001b[39m HailoNNFuser(\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, net_name, converter\u001b[38;5;241m.\u001b[39mend_node_names)\n\u001b[1;32m    368\u001b[0m hailo_nn \u001b[38;5;241m=\u001b[39m fuser\u001b[38;5;241m.\u001b[39mconvert_model()\n\u001b[1;32m    369\u001b[0m hailo_nn\u001b[38;5;241m.\u001b[39mvalidate_stage(HnStage\u001b[38;5;241m.\u001b[39mHN)\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/model_translator/translator.py:83\u001b[0m, in \u001b[0;36mHailoNNConverter.convert_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_model_params()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_bn_ops_in_training()\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_layers_connections()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers_graph\u001b[38;5;241m.\u001b[39mset_names_and_indices()\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/model_translator/edge_nn_translator.py:40\u001b[0m, in \u001b[0;36mEdgeNNConverter._create_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_input_layers()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_vertices_info()\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_direct_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_processed_vertices()\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/model_translator/edge_nn_translator.py:122\u001b[0m, in \u001b[0;36mEdgeNNConverter._add_direct_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m vertex \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_visited_states:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing vertex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvertex\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_layer_callback_from_vertex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_visited_states[vertex] \u001b[38;5;241m=\u001b[39m VertexState\u001b[38;5;241m.\u001b[39mPROCESSED\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39msuccessors(vertex), key\u001b[38;5;241m=\u001b[39mattrgetter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/model_translator/onnx_translator/onnx_translator.py:389\u001b[0m, in \u001b[0;36mONNXConverter._layer_callback_from_vertex\u001b[0;34m(self, vertex)\u001b[0m\n\u001b[1;32m    372\u001b[0m     consumed_vertices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_layer_normalization_layer(vertex, group_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    374\u001b[0m     vertex\u001b[38;5;241m.\u001b[39mis_f_to_w_transpose_reshape()\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m vertex\u001b[38;5;241m.\u001b[39mis_flat_to_frames_reshape()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m vertex\u001b[38;5;241m.\u001b[39mis_partial_groups_to_spatial_flatten()\n\u001b[1;32m    388\u001b[0m ):\n\u001b[0;32m--> 389\u001b[0m     consumed_vertices, should_assign_vertex_to_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_format_conversion_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to determine type of layer to create in node \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvertex\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/model_translator/onnx_translator/onnx_translator.py:1451\u001b[0m, in \u001b[0;36mONNXConverter._create_format_conversion_layer\u001b[0;34m(self, vertex)\u001b[0m\n\u001b[1;32m   1443\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspatial_reshape_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m spatial_reshape_sizes\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1446\u001b[0m     vertex\u001b[38;5;241m.\u001b[39mis_spatial_unflatten()\n\u001b[1;32m   1447\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vertex\u001b[38;5;241m.\u001b[39mis_f_to_w_transpose_reshape()\n\u001b[1;32m   1448\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vertex\u001b[38;5;241m.\u001b[39mis_merge_windows_to_input_reshape_chain()\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vertex\u001b[38;5;241m.\u001b[39mis_split_input_to_windows_reshape_chain()\n\u001b[1;32m   1450\u001b[0m ):\n\u001b[0;32m-> 1451\u001b[0m     consumed_vertices, shapes, expand_sizes \u001b[38;5;241m=\u001b[39m \u001b[43mvertex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_spatial_unflatten_reshape_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1452\u001b[0m     format_conversion \u001b[38;5;241m=\u001b[39m FormatConversionType\u001b[38;5;241m.\u001b[39mspatial_reshape\n\u001b[1;32m   1453\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspatial_reshape_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m expand_sizes\n",
      "File \u001b[0;32m~/Documents/ProjWork1_DFC/hailoDFC/hailodfc/lib/python3.10/site-packages/hailo_sdk_client/model_translator/onnx_translator/onnx_graph.py:3602\u001b[0m, in \u001b[0;36mONNXGraphNode.get_spatial_unflatten_reshape_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedNodeError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find reshape node in format conversion layer near \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3601\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m consumed_vertices[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mget_output_shapes()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 3602\u001b[0m spatial_reshape_sizes \u001b[38;5;241m=\u001b[39m [output_shape[\u001b[38;5;241m1\u001b[39m], \u001b[43moutput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[1;32m   3603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m consumed_vertices, [output_shape], spatial_reshape_sizes\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "runner = ClientRunner(hw_arch=chosen_hw_arch)\n",
    "\n",
    "\n",
    "print(f\"Take model from {model_path_simple}\")\n",
    "hn, npz = runner.translate_onnx_model(\n",
    "    model_path_simple,\n",
    "    model_name\n",
    ")\n",
    "\n",
    "hailo_model_har_name = str(har_path / f\"{model_name}_hailo_model.har\")\n",
    "runner.save_har(hailo_model_har_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Har file\n",
    "\n",
    "The next step is to optimze the har file. To do this we need a calibration dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, PILToTensor\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "\n",
    "datafolder = ptf.dataBaseFolder\n",
    "input_folder = ptf.Dataset5Patch\n",
    "calibFolder = datafolder / \"calibData\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to preprocess the input images to the right resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "def transform(n_px):\n",
    "    \"\"\"\n",
    "    n_px: input resolution of the network\n",
    "    \"\"\"\n",
    "    return Compose([\n",
    "        Resize(n_px, interpolation=BICUBIC),\n",
    "        CenterCrop(n_px),\n",
    "        _convert_image_to_rgb,\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x_y_pixel` should be the input resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_y_pixel = 224\n",
    "preprocess = transform(x_y_pixel)\n",
    "images_list = [img_name for img_name in os.listdir(input_folder) if os.path.splitext(img_name)[1] == \".jpg\"]\n",
    "images_list = images_list[0:1024]\n",
    "calib_dataset = np.zeros((len(images_list), x_y_pixel, x_y_pixel, 3))\n",
    "\n",
    "for idx, img_name in enumerate(sorted(images_list)):\n",
    "    img = Image.open(os.path.join(input_folder, img_name))\n",
    "    # img = PILToTensor(img)\n",
    "    img_preproc = preprocess(img)\n",
    "    img_transposed = np.transpose(img_preproc.numpy(),(1,2,0))\n",
    "    input_data = (img_transposed * 255).astype(np.uint8)  # Assuming image is already normalized\n",
    "    calib_dataset[idx, :, :, :] = img_transposed\n",
    "\n",
    "np.save(calibFolder / f\"calib_set_{model_name}.npy\", calib_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load our parsed HAR from the Parsing Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, we will load our parsed HAR from the Parsing Tutorial\n",
    "\n",
    "hailo_model_har_name = f\"{model_name}_hailo_model.har\"\n",
    "hailo_model_har_path = har_path / hailo_model_har_name\n",
    "assert os.path.isfile(hailo_model_har_path), \"Please provide valid path for HAR file\"\n",
    "runner = ClientRunner(har=str(hailo_model_har_path),hw_arch=chosen_hw_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Starting Model Optimization\n",
      "[warning] Reducing optimization level to 0 (the accuracy won't be optimized and compression won't be used) because there's no available GPU\n",
      "[warning] Running model optimization with zero level of optimization is not recommended for production use and might lead to suboptimal accuracy results\n",
      "[info] Model received quantization params from the hn\n",
      "[info] Starting Mixed Precision\n",
      "[info] Mixed Precision is done (completion time is 00:00:00.01)\n",
      "[info] LayerNorm Decomposition skipped\n",
      "[info] Starting Statistics Collector\n",
      "[info] Using dataset with 64 entries for calibration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibration: 100%|██████████| 64/64 [00:00<00:00, 85.09entries/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Statistics Collector is done (completion time is 00:00:00.78)\n",
      "[info] Starting Fix zp_comp Encoding\n",
      "[info] Fix zp_comp Encoding is done (completion time is 00:00:00.00)\n",
      "[info] Matmul Equalization skipped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Finetune encoding skipped\n",
      "[info] Bias Correction skipped\n",
      "[info] Adaround skipped\n",
      "[info] Quantization-Aware Fine-Tuning skipped\n",
      "[info] Layer Noise Analysis skipped\n",
      "[info] The calibration set seems to not be normalized, because the values range is [(-1.7922626, 1.9303361), (-1.7520971, 2.0748837), (-1.4802198, 2.145897)].\n",
      "Since the neural core works in 8-bit (between 0 to 255), a quantization will occur on the CPU of the runtime platform.\n",
      "Add a normalization layer to the model to offload the normalization to the neural core.\n",
      "Refer to the user guide Hailo Dataflow Compiler user guide / Model Optimization / Optimization Related Model Script Commands / model_modification_commands / normalization for details.\n",
      "[info] Model Optimization is done\n",
      "[info] Saved HAR to: /home/lukasschoepf/Documents/ProjWork1_DFC/hailoDFC/models/Harfiles/superSmall_CLIP_RN50_quantized_model.har\n",
      "saved model at hailoDFC/models/Harfiles/superSmall_CLIP_RN50_quantized_model.har\n"
     ]
    }
   ],
   "source": [
    "# Batch size is 8 by default\n",
    "# alls = \"normalization1 = normalization([123.675, 116.28, 103.53], [58.395, 57.12, 57.375])\\n\" # From tutorial\n",
    "\n",
    "# Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)), # From Lia\n",
    "#alls = \"normalization1 = normalization([0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711])\\n\"\n",
    "\n",
    "# Load the model script to ClientRunner so it will be considered on optimization\n",
    "# runner.load_model_script(alls)\n",
    "\n",
    "# Call Optimize to perform the optimization process\n",
    "runner.optimize(calib_dataset)\n",
    "\n",
    "# Save the result state to a Quantized HAR file\n",
    "quantized_model_har_path = str(har_path / f\"{model_name}_quantized_model.har\")\n",
    "runner.save_har(quantized_model_har_path)\n",
    "print(f\"saved model at {quantized_model_har_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the quantized model\n",
    "\n",
    "Now we can compile the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used:superSmall_CLIP_RN50_quantized_model.har\n",
      "[info] To achieve optimal performance, set the compiler_optimization_level to \"max\" by adding performance_param(compiler_optimization_level=max) to the model script. Note that this may increase compilation time.\n",
      "[info] Loading network parameters\n",
      "[info] Starting Hailo allocation and compilation flow\n",
      "[info] Using Single-context flow\n",
      "[info] Resources optimization guidelines: Strategy -> GREEDY Objective -> MAX_FPS\n",
      "[info] Resources optimization params: max_control_utilization=75%, max_compute_utilization=75%, max_compute_16bit_utilization=75%, max_memory_utilization (weights)=75%, max_input_aligner_utilization=75%, max_apu_utilization=75%\n",
      "[info] Using Single-context flow\n",
      "[info] Resources optimization guidelines: Strategy -> GREEDY Objective -> MAX_FPS\n",
      "[info] Resources optimization params: max_control_utilization=75%, max_compute_utilization=75%, max_compute_16bit_utilization=75%, max_memory_utilization (weights)=75%, max_input_aligner_utilization=75%, max_apu_utilization=75%\n",
      "[info] output_layer1: Pass\n",
      "[info] input_layer1: Pass\n",
      "[info] auto_reshape_from_input_layer1_to_conv1_sd0-1: Pass\n",
      "[info] conv2_sdc: Pass\n",
      "[info] smuffers_shortcut_conv2_to_conv3: Pass\n",
      "[info] conv2_sd3: Pass\n",
      "[info] conv3_sd1: Pass\n",
      "[info] conv1_sd1: Pass\n",
      "[info] sh_from_smuffers_shortcut_conv2_to_conv3_to_conv3_sd0-3: Pass\n",
      "[info] conv3_sd3: Pass\n",
      "[info] conv3_sd2: Pass\n",
      "[info] conv1_sdc: Pass\n",
      "[info] conv1_sd0: Pass\n",
      "[info] conv2_sd2: Pass\n",
      "[info] sh_from_smuffers_shortcut_conv2_to_conv3_to_conv3_sd4-7: Pass\n",
      "[info] conv3_sd0: Pass\n",
      "[info] concat_from_conv3_sd4-7_to_conv3_sdc: Pass\n",
      "[info] conv3_sd4: Pass\n",
      "[info] conv3_sd6: Pass\n",
      "[info] conv2_sd1: Pass\n",
      "[info] conv2_sd0: Pass\n",
      "[info] concat_from_conv3_sd0-3_to_conv3_sdc: Pass\n",
      "[info] conv3_sd7: Pass\n",
      "[info] conv3_sd5: Pass\n",
      "[info] conv3_sdc: Pass\n",
      "[info] Solving the allocation (Mapping), time per context: 59m 59s\n",
      "\n",
      "\n",
      "[info] Context:0/0 Iteration 0: Mapping prepost...          \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  *          *          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2                                                                                                  \n",
      " worker3                                                                                                  \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 0: Trying parallel splits...   \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0                                                                                                  \n",
      " worker1                                                                                                  \n",
      " worker2                                                                                                  \n",
      " worker3                                                                                                  \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 0: Trying parallel splits...   \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0                                                                                                  \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3                                                                                                  \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 0: Trying parallel splits...   \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0                                                                                                  \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          *          *          *          *          *          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 0: Trying parallel splits...   \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0                                                                                                  \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          *          *          *          *          *          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 0: Trying parallel splits...   \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0                                                                                                  \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 4: Trying parallel mapping...  \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  V          *          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 4: Trying parallel mapping...  \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  V          *          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 4: Trying parallel mapping...  \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  V          *          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 4: Trying parallel mapping...  \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  V          V          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 4: Trying parallel mapping...  \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  V          V          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 4: Trying parallel mapping...  \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  V          V          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 4: Trying parallel mapping...  \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  V          V          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 4: Trying parallel mapping...  \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  V          V          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 4: Trying parallel mapping...  \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  V          V          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 4: Trying parallel mapping...  \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  V          V          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 4: Trying parallel mapping...  \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  V          V          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 4: Trying parallel mapping...  \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  V          V          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 4: Trying parallel mapping...  \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  V          V          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2  *          *          *          *          *          *          *          *          V       \n",
      " worker3  *          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:00\n",
      "[info] Context:0/0 Iteration 4: Trying parallel mapping...  \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  V          V          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2  V          V          *          *          *          *          *          *          V       \n",
      " worker3  *          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:01\n",
      "[info] Context:0/0 Iteration 4: Trying parallel mapping...  \n",
      "          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost \n",
      " worker0  V          V          *          *          *          *          *          *          V       \n",
      " worker1                                                                                                  \n",
      " worker2  V          V          *          *          *          *          *          *          V       \n",
      " worker3  V          V          *          *          V          V          *          *          V       \n",
      "\n",
      "  00:01\n",
      "\n",
      "[info] Iterations: 4\n",
      "Reverts on cluster mapping: 0\n",
      "Reverts on inter-cluster connectivity: 0\n",
      "Reverts on pre-mapping validation: 0\n",
      "Reverts on split failed: 0\n",
      "[info] +-----------+---------------------+---------------------+--------------------+\n",
      "[info] | Cluster   | Control Utilization | Compute Utilization | Memory Utilization |\n",
      "[info] +-----------+---------------------+---------------------+--------------------+\n",
      "[info] | cluster_0 | 87.5%               | 87.5%               | 28.9%              |\n",
      "[info] | cluster_1 | 50%                 | 56.3%               | 18%                |\n",
      "[info] +-----------+---------------------+---------------------+--------------------+\n",
      "[info] | Total     | 34.4%               | 35.9%               | 11.7%              |\n",
      "[info] +-----------+---------------------+---------------------+--------------------+\n",
      "[info] Successful Mapping (allocation time: 4s)\n",
      "[info] Compiling context_0...\n",
      "[info] Bandwidth of model inputs: 1.14844 Mbps, outputs: 6.125 Mbps (for a single frame)\n",
      "[info] Bandwidth of DDR buffers: 0.0 Mbps (for a single frame)\n",
      "[info] Bandwidth of inter context tensors: 0.0 Mbps (for a single frame)\n",
      "[info] Compiling context_0...\n",
      "[info] Bandwidth of model inputs: 1.14844 Mbps, outputs: 6.125 Mbps (for a single frame)\n",
      "[info] Bandwidth of DDR buffers: 0.0 Mbps (for a single frame)\n",
      "[info] Bandwidth of inter context tensors: 0.0 Mbps (for a single frame)\n",
      "[info] Building HEF...\n",
      "[info] Successful Compilation (compilation time: 0s)\n",
      "[info] Saved HAR to: /home/lukasschoepf/Documents/ProjWork1_DFC/hailoDFC/models/Harfiles/superSmall_CLIP_RN50_compiled_model.har\n"
     ]
    }
   ],
   "source": [
    "quantized_model_har_path = str(har_path / f\"{model_name}_quantized_model.har\")\n",
    "print(f\"Model used:{model_name}_quantized_model.har\")\n",
    "runner = ClientRunner(har=quantized_model_har_path,hw_arch=chosen_hw_arch)\n",
    "# By default it uses the hw_arch that is saved on the HAR. It is not recommended to change the hw_arch after Optimization.\n",
    "\n",
    "hef = runner.compile()\n",
    "\n",
    "file_name = str(hef_path / f\"{model_name}.hef\")\n",
    "with open(file_name, \"wb\") as f:\n",
    "    f.write(hef)\n",
    "\n",
    "har_path =ptf.HarPath/ f\"{model_name}_compiled_model.har\"\n",
    "runner.save_har(har_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hailodfc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
